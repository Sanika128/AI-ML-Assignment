{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Title & Objective"
      ],
      "metadata": {
        "id": "i1jAXfJgugA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Conversation Management & Classification using Groq API\n",
        "\n",
        "**Objective:**  \n",
        "Implement two core tasks using Groq APIs (OpenAI-compatible) without frameworks:\n",
        "\n",
        "1. **Task 1:** Manage conversation history with summarization and truncation.  \n",
        "2. **Task 2:** Extract structured information from chats using JSON schema and function calling.\n"
      ],
      "metadata": {
        "id": "4hwzjaMHu3Ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports & Setup"
      ],
      "metadata": {
        "id": "G8WlrTQhvCaq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLqmCcQNt7tZ",
        "outputId": "13292f15-4de9-4e91-83d2-ea001b76bc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Secure API key setup ---\n",
            "Enter your GROQ API key (input hidden): ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import textwrap\n",
        "from getpass import getpass\n",
        "from typing import List, Dict, Any, Optional\n",
        "import requests\n",
        "\n",
        "try:\n",
        "    import jsonschema\n",
        "except Exception:\n",
        "    print(\"jsonschema not found. Installing...\")\n",
        "    !pip install jsonschema\n",
        "    import jsonschema\n",
        "\n",
        "print(\"--- Secure API key setup ---\")\n",
        "GROQ_API_KEY = getpass('Enter your GROQ API key (input hidden): ')\n",
        "if not GROQ_API_KEY:\n",
        "    raise ValueError(\"API key is required to run demo calls. Set GROQ_API_KEY as an environment variable or paste it when asked.\")\n",
        "\n",
        "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Groq API Wrapper"
      ],
      "metadata": {
        "id": "hrvpUqxquaE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Groq OpenAI-compatible API wrapper (copy-paste this entire cell) ---\n",
        "import os, json, re, requests\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# Use the OpenAI-compatible Groq base path\n",
        "GROQ_API_BASE = os.environ.get('GROQ_API_BASE', 'https://api.groq.com/openai/v1')\n",
        "CHAT_COMPLETIONS_URL = f\"{GROQ_API_BASE}/chat/completions\"\n",
        "\n",
        "# Default model: change this to any model id from your \"Available models\" list\n",
        "DEFAULT_MODEL = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "def _realistic_mock_response(messages, functions):\n",
        "    \"\"\"\n",
        "    Heuristic/mock fallback: attempts simple regex extraction so demos look realistic\n",
        "    when a real API call fails or no key is provided.\n",
        "    \"\"\"\n",
        "    full_text = \" \".join([m.get('content','') for m in messages]).strip()\n",
        "    name_match = re.search(r\"(?:I am|I'm|Name:|this is)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\", full_text)\n",
        "    email_match = re.search(r\"([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})\", full_text)\n",
        "    phone_match = re.search(r\"(\\+?\\d{1,3}[\\s-]?\\d{3,}[\\d\\s-]{3,})\", full_text)\n",
        "    location_match = re.search(r\"(?:based in|Location:|in)\\s+([A-Za-z][A-Za-z\\s-]{1,50}?)(?:[.,;]|$)\", full_text)\n",
        "    age_match = re.search(r\"(?:I am|I'm|age[:\\s]*)\\s*(\\d{1,2})(?:\\s*(?:years old|yrs|y/o))?\", full_text, flags=re.IGNORECASE)\n",
        "\n",
        "    fake_args = {}\n",
        "    if name_match: fake_args['name'] = name_match.group(1).strip()\n",
        "    if email_match: fake_args['email'] = email_match.group(1).strip()\n",
        "    if phone_match: fake_args['phone'] = phone_match.group(1).strip()\n",
        "    if location_match: fake_args['location'] = location_match.group(1).strip().rstrip('.')\n",
        "    if age_match:\n",
        "        try: fake_args['age'] = int(age_match.group(1))\n",
        "        except: pass\n",
        "\n",
        "    defaults = {\"name\":\"Unknown\",\"email\":\"not_provided@example.com\",\"phone\":\"not_provided\",\"location\":\"not_provided\",\"age\":None}\n",
        "    for k,v in defaults.items(): fake_args.setdefault(k,v)\n",
        "\n",
        "    if functions:\n",
        "        return {\"choices\":[{\"message\":{\"function_call\":{\"name\": functions[0].get('name','extract_user_info'), \"arguments\": json.dumps(fake_args)}}}]}\n",
        "    else:\n",
        "        return {\"choices\":[{\"message\":{\"content\":\"MOCK SUMMARY: (demo) summary placeholder.\"}}]}\n",
        "\n",
        "def groq_chat_completion(messages: List[Dict[str, str]],\n",
        "                         functions: Optional[List[Dict[str, Any]]] = None,\n",
        "                         function_call: Optional[Any] = 'auto',\n",
        "                         model: str = DEFAULT_MODEL,\n",
        "                         max_tokens: int = 800,\n",
        "                         timeout: int = 15) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Robust Groq/OpenAI-compatible chat wrapper.\n",
        "\n",
        "    - Uses /openai/v1 path (correct for Groq's OpenAI-compatible API).\n",
        "    - If GROQ_API_KEY present in environment, attempts real API call.\n",
        "    - On HTTP error or other failures, prints diagnostics and falls back to a realistic mock response.\n",
        "    - `functions` and `function_call` are passed through (for function-calling flows).\n",
        "    \"\"\"\n",
        "    payload = {'model': model, 'messages': messages, 'max_tokens': max_tokens}\n",
        "    if functions is not None:\n",
        "        payload['functions'] = functions\n",
        "        payload['function_call'] = function_call\n",
        "\n",
        "    key = os.environ.get('GROQ_API_KEY','')\n",
        "    if not key:\n",
        "        print(\"⚠️ No GROQ_API_KEY found in environment — running MOCK mode.\")\n",
        "        return _realistic_mock_response(messages, functions)\n",
        "\n",
        "    headers = {'Authorization': f\"Bearer {key}\", 'Content-Type': 'application/json'}\n",
        "    try:\n",
        "        resp = requests.post(CHAT_COMPLETIONS_URL, headers=headers, json=payload, timeout=timeout)\n",
        "        resp.raise_for_status()\n",
        "        return resp.json()\n",
        "    except requests.HTTPError as http_err:\n",
        "        code = getattr(http_err.response,'status_code',None)\n",
        "        print(f\"⚠️ HTTP error calling Groq API: {http_err} (status {code})\")\n",
        "        try:\n",
        "            body = http_err.response.text\n",
        "            snippet = (body[:1000] + \"... [truncated]\") if len(body) > 1000 else body\n",
        "            print(\"Server response (truncated):\", snippet)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"Falling back to realistic MOCK response so the notebook can continue.\")\n",
        "        return _realistic_mock_response(messages, functions)\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Error calling Groq API:\", e)\n",
        "        print(\"Falling back to realistic MOCK response so the notebook can continue.\")\n",
        "        return _realistic_mock_response(messages, functions)\n",
        "\n",
        "# Quick self-check (optional) — run this cell and examine printed output\n",
        "if __name__ == \"__main__\":\n",
        "    _test_msgs = [{'role':'user','content':\"Self-check: say hello.\"}]\n",
        "    print(\"Wrapper ready. Self-check result (may be mock if key/model not available):\")\n",
        "    print(groq_chat_completion(_test_msgs))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA134iEzvplB",
        "outputId": "d877c8e3-1c0a-43b7-f20f-8c5a2e202e1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapper ready. Self-check result (may be mock if key/model not available):\n",
            "{'id': 'chatcmpl-83188699-d6a0-44a6-a5e3-b21a4b1c86b1', 'object': 'chat.completion', 'created': 1757945914, 'model': 'llama-3.3-70b-versatile', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Hello.'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'queue_time': 0.04630326, 'prompt_tokens': 41, 'prompt_time': 0.01100546, 'completion_tokens': 3, 'completion_time': 2.4e-07, 'total_tokens': 44, 'total_time': 0.0110057}, 'usage_breakdown': None, 'system_fingerprint': 'fp_2ddfbb0da0', 'x_groq': {'id': 'req_01k56vvrq8e97t8jqqk777qwsg'}, 'service_tier': 'on_demand'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. ConversationManager Class"
      ],
      "metadata": {
        "id": "u3sHnUK8vv0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Updated ConversationManager (use the same DEFAULT_MODEL as wrapper) ---\n",
        "class ConversationManager:\n",
        "    def __init__(self, periodic_k: int = 3, summarization_model: str = DEFAULT_MODEL):\n",
        "        \"\"\"\n",
        "        periodic_k: perform summarization after every k runs\n",
        "        summarization_model: model id to use for summarization (defaults to DEFAULT_MODEL)\n",
        "        \"\"\"\n",
        "        self.history = []  # list of {'role':..., 'content':...}\n",
        "        self.run_count = 0\n",
        "        self.periodic_k = periodic_k\n",
        "        self.summarization_model = summarization_model\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        self.history.append({'role': role, 'content': content})\n",
        "\n",
        "    def last_n_turns(self, n: int):\n",
        "        return self.history[-n:]\n",
        "\n",
        "    def truncate_by_chars(self, max_chars: int):\n",
        "        out = []\n",
        "        total = 0\n",
        "        for msg in reversed(self.history):\n",
        "            msg_len = len(msg['content'])\n",
        "            if total + msg_len > max_chars:\n",
        "                break\n",
        "            out.insert(0, msg)\n",
        "            total += msg_len\n",
        "        return out\n",
        "\n",
        "    def summarize_history(self, prompt_extra: Optional[str] = None) -> str:\n",
        "        history_text = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in self.history])\n",
        "        system_msg = {\n",
        "            'role': 'system',\n",
        "            'content': 'You are a helpful summarizer. Produce a short, bullet or paragraph summary of the conversation focusing on facts and user needs.'\n",
        "        }\n",
        "        user_msg = {\n",
        "            'role': 'user',\n",
        "            'content': 'Summarize the conversation below into 2-4 short bullet points.\\n\\nConversation:\\n' + history_text + (('\\n\\n' + prompt_extra) if prompt_extra else '')\n",
        "        }\n",
        "        # explicitly pass model so it doesn't use an outdated default\n",
        "        resp = groq_chat_completion([system_msg, user_msg], model=self.summarization_model, max_tokens=300)\n",
        "        try:\n",
        "            return resp['choices'][0]['message']['content']\n",
        "        except Exception:\n",
        "            return str(resp)\n",
        "\n",
        "    def maybe_periodic_summarize(self):\n",
        "        self.run_count += 1\n",
        "        print(f\"Run count: {self.run_count} (periodic_k={self.periodic_k})\")\n",
        "        if self.periodic_k > 0 and self.run_count % self.periodic_k == 0:\n",
        "            print(\"Performing periodic summarization...\")\n",
        "            summary = self.summarize_history()\n",
        "            self.history = [{'role': 'system', 'content': 'Summary of previous conversation: ' + summary}]\n",
        "            return summary\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "BcaLf75Kv5cZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Task 1"
      ],
      "metadata": {
        "id": "5C0V36lRv-cD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_task1():\n",
        "    cm = ConversationManager(periodic_k=3)\n",
        "    samples = [\n",
        "        (\"user\", \"Hi, I'm Sanika. I'm looking for internship advice.\"),\n",
        "        (\"assistant\", \"Sure — what field are you interested in?\"),\n",
        "        (\"user\", \"AI/ML roles, but I have more Python than ML projects.\"),\n",
        "        (\"assistant\", \"You can start with small projects: ...\"),\n",
        "        (\"user\", \"Also I want to improve my resume formatting.\"),\n",
        "        (\"assistant\", \"Add project bullets, quantify impact, use clear headers.\"),\n",
        "    ]\n",
        "\n",
        "    print('\\nFeeding samples and showing truncation settings...')\n",
        "    for role, text in samples:\n",
        "        cm.add_message(role, text)\n",
        "\n",
        "    print('\\n-- Last 3 turns --')\n",
        "    for m in cm.last_n_turns(3):\n",
        "        print(f\"{m['role']}: {m['content']}\")\n",
        "\n",
        "    print('\\n-- Truncate by 120 chars --')\n",
        "    truncated = cm.truncate_by_chars(120)\n",
        "    for m in truncated:\n",
        "        print(f\"{m['role']}: {m['content']}\")\n",
        "\n",
        "    print('\\n-- Demonstrate periodic summarization after every 3 runs --')\n",
        "    for i in range(1, 7):\n",
        "        cm.add_message('user', f'Follow-up message #{i}')\n",
        "        summary = cm.maybe_periodic_summarize()\n",
        "        if summary:\n",
        "            print('Summary created:')\n",
        "            print(summary)\n",
        "            print('History now contains:')\n",
        "            print(cm.history)\n",
        "\n",
        "demo_task1()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlKyJ7iSwGKQ",
        "outputId": "dce8e3e9-2cec-46ea-b9d0-dffc4f8481aa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feeding samples and showing truncation settings...\n",
            "\n",
            "-- Last 3 turns --\n",
            "assistant: You can start with small projects: ...\n",
            "user: Also I want to improve my resume formatting.\n",
            "assistant: Add project bullets, quantify impact, use clear headers.\n",
            "\n",
            "-- Truncate by 120 chars --\n",
            "user: Also I want to improve my resume formatting.\n",
            "assistant: Add project bullets, quantify impact, use clear headers.\n",
            "\n",
            "-- Demonstrate periodic summarization after every 3 runs --\n",
            "Run count: 1 (periodic_k=3)\n",
            "Run count: 2 (periodic_k=3)\n",
            "Run count: 3 (periodic_k=3)\n",
            "Performing periodic summarization...\n",
            "Summary created:\n",
            "Here is a summary of the conversation in 3 bullet points:\n",
            "\n",
            "* Sanika is looking for internship advice, specifically in AI/ML roles.\n",
            "* Sanika has more Python projects than ML projects, and may need to start with small projects to build experience.\n",
            "* Sanika also wants to improve their resume formatting, with suggestions including adding project bullets, quantifying impact, and using clear headers.\n",
            "History now contains:\n",
            "[{'role': 'system', 'content': 'Summary of previous conversation: Here is a summary of the conversation in 3 bullet points:\\n\\n* Sanika is looking for internship advice, specifically in AI/ML roles.\\n* Sanika has more Python projects than ML projects, and may need to start with small projects to build experience.\\n* Sanika also wants to improve their resume formatting, with suggestions including adding project bullets, quantifying impact, and using clear headers.'}]\n",
            "Run count: 4 (periodic_k=3)\n",
            "Run count: 5 (periodic_k=3)\n",
            "Run count: 6 (periodic_k=3)\n",
            "Performing periodic summarization...\n",
            "Summary created:\n",
            "Here is a summary of the conversation in 3 bullet points:\n",
            "\n",
            "* Sanika is seeking advice on internships, particularly in AI/ML roles.\n",
            "* Sanika may need to build experience through small projects, as their current portfolio has more Python projects than ML projects.\n",
            "* Sanika also aims to enhance their resume by adding project details, quantifying accomplishments, and improving overall formatting.\n",
            "History now contains:\n",
            "[{'role': 'system', 'content': 'Summary of previous conversation: Here is a summary of the conversation in 3 bullet points:\\n\\n* Sanika is seeking advice on internships, particularly in AI/ML roles.\\n* Sanika may need to build experience through small projects, as their current portfolio has more Python projects than ML projects.\\n* Sanika also aims to enhance their resume by adding project details, quantifying accomplishments, and improving overall formatting.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Task 2: JSON Schema Extraction"
      ],
      "metadata": {
        "id": "IpJeRBScwL2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FUNCTION_SCHEMA = {\n",
        "    'name': 'extract_user_info',\n",
        "    'description': 'Extract user contact and personal info from a chat message',\n",
        "    'parameters': {\n",
        "        'type': 'object',\n",
        "        'properties': {\n",
        "            'name': {'type': 'string'},\n",
        "            'email': {'type': 'string'},\n",
        "            'phone': {'type': 'string'},\n",
        "            'location': {'type': 'string'},\n",
        "            'age': {'type': 'integer'}\n",
        "        },\n",
        "        'required': []\n",
        "    }\n",
        "}\n",
        "\n",
        "def call_extraction_function(chat_text):\n",
        "    system_msg = {'role': 'system', 'content': 'You are a precise extractor. Return only JSON following the schema.'}\n",
        "    user_msg = {'role': 'user', 'content': 'Extract contact info: ' + chat_text}\n",
        "    resp = groq_chat_completion([system_msg, user_msg], functions=[FUNCTION_SCHEMA], function_call={'name': 'extract_user_info'})\n",
        "    fc = resp['choices'][0]['message'].get('function_call')\n",
        "    if fc and 'arguments' in fc:\n",
        "        return json.loads(fc['arguments'])\n",
        "    return {}\n",
        "\n",
        "SAMPLE_CHATS = [\n",
        "    \"Hey I'm Priya Sharma. You can reach me at priya.sharma@example.com or call me on +91 98765 43210. I'm based in Pune and I'm 24.\",\n",
        "    \"Hello, this is Amit. Email: amit123@mail.com. Location: Mumbai. Phone: 022-555-1234.\",\n",
        "    \"Hi, I am 30 and live in Bangalore. Contact me at 9876543210. Name: Rahul.\"\n",
        "]\n",
        "\n",
        "for chat in SAMPLE_CHATS:\n",
        "    print(\"\\nChat:\", chat)\n",
        "    print(\"Extracted:\", call_extraction_function(chat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lY-TCVnwTIY",
        "outputId": "d13391e1-9ab2-4bae-e4bc-ef4db721728e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chat: Hey I'm Priya Sharma. You can reach me at priya.sharma@example.com or call me on +91 98765 43210. I'm based in Pune and I'm 24.\n",
            "Extracted: {'age': 24, 'email': 'priya.sharma@example.com', 'location': 'Pune', 'name': 'Priya Sharma', 'phone': '+91 98765 43210'}\n",
            "\n",
            "Chat: Hello, this is Amit. Email: amit123@mail.com. Location: Mumbai. Phone: 022-555-1234.\n",
            "Extracted: {'email': 'amit123@mail.com', 'location': 'Mumbai', 'name': 'Amit', 'phone': '022-555-1234'}\n",
            "\n",
            "Chat: Hi, I am 30 and live in Bangalore. Contact me at 9876543210. Name: Rahul.\n",
            "Extracted: {'age': 30, 'location': 'Bangalore', 'name': 'Rahul', 'phone': '9876543210'}\n"
          ]
        }
      ]
    }
  ]
}